{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><i>Big Data Analytics - Práctica Calificada 1</i></center>\n",
    "# <center>Bloque 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre: Yair Salinas Bolaños"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ejercicio 1:</b> **[5 puntos]** <h2>Promedio de letras por palabra</h2>\n",
    "\n",
    "Empleando la plataforma **CentOS**, realizar los siguientes procedimientos:\n",
    "\n",
    "    1. Descargar un archivo TXT (libroPC1.txt) desde la plataforma de Project Gutenberg, el archivo debe almacenarse en el escritorio (Desktop)\n",
    "\n",
    "    2. Reportar el/los comando/s para copiar el archivo (libro.txt) desde el escritorio hasta la siguiente ruta: /home/cloudera/BDA/PC1/ArchivosTXT/libroPC1.txt Se debe usar comandos para la creación de las carpetas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Respuesta (comando/s)\n",
    "1) mv libroPC1.txt ../Desktop   # Mover de la carpeta Downloads a Desktop\n",
    "2) cd ..   # del Desktop a cloudera\n",
    "   mkdir  BDA\n",
    "   cd BDA/ \n",
    "   mkdir PC1\n",
    "   cd PC1/\n",
    "   mkdir ArchivosTXT\n",
    "   cd    # volvemos a home/cloudera\n",
    "   cp libroPC1.txt  BDA/PC1/ArchivosTXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Reportar el/los comando/s para copiar el archivo libroPC1.txt hacia HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Respuesta (comando/s)\n",
    "sudo bash\n",
    "cd BDA/PC1/ArchivosTXT\n",
    "hadoop fs -copyFromLocal  libroPC1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Aplicando Map-reduce ejecutar el script respectivo para obtener el promedio de la cantidad de letras por palabra. Esto debe ser ejecutado en HDFS y el archivo resultante debe ser enviado a FS local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Respuesta (Comando/s) - \n",
    "\n",
    "hadoop jar /usr/jars/hadoop-examples.jar\n",
    "\n",
    "# Como parte de la respuesta de esta pregunta se debe también subir el archivo \n",
    "# empleado para el procesamiento y el archivo de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ejercicio 3:</b> **[7 puntos]** <h2>Creando/Eliminando Directorios</h2>\n",
    "\n",
    "Empleando el archivo **Datos_Spam.txt** disponible en el aula virtual, realizar las siguientes operaciones:\n",
    "\n",
    "* Crear un archivo HAM_Total.txt con el contenido de todos los mensajes que son HAM. Realizar el mismo procedimiento para SPAM.\n",
    "\n",
    "* Crear un directorio \"Files\", dentro de este directorio se debe crear una carpeta por cada mensaje (ham/spam), los nombres de las carpetas pueden ser: ham1, ham2, ham3, etc.\n",
    "\n",
    "* Dentro de cada carpeta creada, guardar un archivo reporte.txt, con la siguiente información:\n",
    "\n",
    "    * Cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code\n",
    "## Parte 1:\n",
    "arc = open('Datos_SPAM.txt',mode='r',encoding='utf-8')  \n",
    "cad=arc.readlines() \n",
    "\n",
    "arc2 = open(\"HAM_Total.txt\",mode='w',encoding='utf-8')  \n",
    "arc3 = open(\"SPAM_Total.txt\",mode='w',encoding='utf-8')\n",
    "for i in range (len(cad)):\n",
    "    if 'ham' in cad[i]:\n",
    "        arc2.write(cad[i])  \n",
    "    if 'spam' in cad[i]:\n",
    "        arc3.write(cad[i]) \n",
    "\n",
    "\n",
    "arc.close()\n",
    "arc2.close()\n",
    "arc3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 2:\n",
    "import os\n",
    "os.mkdir(\"Files\")\n",
    "arc5 = open('HAM_Total.txt',mode='r',encoding='utf-8')  \n",
    "cad5=arc5.readlines()\n",
    "arc6 = open('SPAM_Total.txt',mode='r',encoding='utf-8')  \n",
    "cad6=arc6.readlines()\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\Yair\\\\Big data\\\\Files')\n",
    "for i in range(len(cad5)):\n",
    "    os.mkdir(\"ham\"+str(i+1))\n",
    "for i in range(len(cad6)):\n",
    "    os.mkdir(\"spam\"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parte 3:\n",
    "os.chdir('C:\\\\Users\\\\Yair\\\\Big data\\\\Files')\n",
    "for i in range (len(cad5)+len(cad6)):\n",
    "    arc = open('HAM_Total.txt',mode='r',encoding='utf-8')  \n",
    "    cad=arc.readlines()\n",
    "    cant=len(cad[i])\n",
    "    nuevo=open('report.txt',mode='w',encoding='utf-8')\n",
    "    nuevo.write(cant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linea=\"sass sa ss ee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linea=linea.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sass', 'sa', 'ss', 'ee']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sa', 'ss', 'ee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linea[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s sa ss ee'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linea[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sass sa ss ee'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linea.spli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(linea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
